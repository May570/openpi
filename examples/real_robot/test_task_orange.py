#!/usr/bin/env python3
"""
task_orangeæ¨ç†æµ‹è¯•ç¨‹åº
ä»çœŸå®æ•°æ®æ–‡ä»¶è¯»å–æ•°æ®æ¥æµ‹è¯•OpenPIæ¨¡å‹çš„æ¨ç†åŠŸèƒ½
"""

import os
import sys
import json
import time
import numpy as np
import requests
import pandas as pd
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TaskTester:
    """task_orangeæ¨ç†æµ‹è¯•å™¨"""
    
    def __init__(self, service_url="http://127.0.0.1:5002", data_file="/home/admin123/Desktop/pi05_orange/episode_000003.parquet", test_interval=50, output_file="all_inference_results.json", predicted_actions_file="predicted_actions.npy", true_actions_file="true_actions.npy"):
        self.service_url = service_url
        self.data_file = data_file
        self.test_interval = test_interval  # æµ‹è¯•é—´éš”ï¼ˆå¸§æ•°ï¼‰
        self.output_file = output_file  # æ¨ç†ç»“æœè¾“å‡ºæ–‡ä»¶å
        self.predicted_actions_file = predicted_actions_file  # é¢„æµ‹actionæ•°ç»„æ–‡ä»¶å
        self.true_actions_file = true_actions_file  # çœŸå®actionæ•°ç»„æ–‡ä»¶å
        self.test_data = None
        self.true_action = None
        self.df = None  # ä¿å­˜æ•´ä¸ªæ•°æ®æ¡†
        self.test_results = []  # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
        self.inference_results = []  # å­˜å‚¨æ‰€æœ‰æ¨ç†ç»“æœ
        self._load_real_data()
    
    def _load_real_data(self):
        """ä»parquetæ–‡ä»¶åŠ è½½çœŸå®çš„task_orangeæ•°æ®"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {self.data_file}")
            
            if not os.path.exists(self.data_file):
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            
            # è¯»å–parquetæ–‡ä»¶
            self.df = pd.read_parquet(self.data_file)
            logger.info(f"æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œå…±{len(self.df)}è¡Œæ•°æ®")
            
            # è®¡ç®—æµ‹è¯•ç‚¹
            self.test_points = self._calculate_test_points()
            logger.info(f"è®¡ç®—å¾—åˆ°{len(self.test_points)}ä¸ªæµ‹è¯•ç‚¹")
            
            # æ„å»ºç¬¬ä¸€ä¸ªæµ‹è¯•ç‚¹çš„æ•°æ®ï¼ˆç”¨äºåˆå§‹åŒ–ï¼‰
            first_row_data = self.df.iloc[self.test_points[0]]
            self.test_data = self._build_test_data_from_row(first_row_data, self.test_points[0])
            logger.info("æµ‹è¯•æ•°æ®æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _build_test_data_from_row(self, row_data, row_index):
        """ä»æ•°æ®è¡Œæ„å»ºæµ‹è¯•æ•°æ®"""
        try:
            # æå–çŠ¶æ€æ•°æ®
            if 'observation.state' in row_data:
                qpos = row_data['observation.state']
                if isinstance(qpos, (list, np.ndarray)):
                    qpos = [qpos] if len(qpos.shape) == 1 else qpos.tolist()
            else:
                print("observation.state not found")
                return None
            
            images = {}
            
            cam_mapping = {
                'observation.images.cam_high': 'cam_high',
                'observation.images.cam_left_wrist': 'cam_left_wrist', 
                'observation.images.cam_right_wrist': 'cam_right_wrist'
            }
            
            for parquet_key, api_key in cam_mapping.items():
                if parquet_key in row_data and pd.notna(row_data[parquet_key]['bytes']):
                    # å¤„ç†å›¾ç‰‡æ•°æ® - å°†bytesè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
                    if isinstance(row_data[parquet_key]['bytes'], bytes):
                        # å¦‚æœæ˜¯bytesç±»å‹ï¼Œè¿›è¡Œbase64ç¼–ç 
                        import base64
                        images[api_key] = base64.b64encode(row_data[parquet_key]['bytes']).decode('utf-8')
                    elif isinstance(row_data[parquet_key]['bytes'], str):
                        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                        images[api_key] = row_data[parquet_key]['bytes']
                    else:
                        print(f"å›¾ç‰‡æ•°æ®æ ¼å¼é”™è¯¯: {parquet_key}, ç±»å‹: {type(row_data[parquet_key]['bytes'])}")
                        return None

            
            # æ„å»ºæœ€ç»ˆçš„æµ‹è¯•æ•°æ®
            test_data = {
                "state": qpos,
                "eef_pose": [[0.0] * 7],  # æ·»åŠ å¿…éœ€çš„eefposeå­—æ®µï¼Œ7ç»´ä½å§¿
                "images": images,
                "prompt": "pick up the orange and put it into the basket"
            }
            
            # æå–çœŸå®çš„actionæ•°æ®ç”¨äºå¯¹æ¯”
            self.true_action = self._extract_action_sequence(row_index)
            if self.true_action is not None:
                logger.info(f"æå–çœŸå®actionæ•°æ®ï¼Œç»´åº¦: {len(self.true_action)} x {len(self.true_action[0]) if self.true_action else 0}")
            else:
                logger.warning("æ— æ³•æå–actionåºåˆ—ï¼Œæ— æ³•è¿›è¡ŒMSEå¯¹æ¯”")
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            test_data = self._ensure_json_serializable(test_data)
            
            logger.info(f"æ„å»ºçš„æµ‹è¯•æ•°æ®åŒ…å«:")
            logger.info(f"  - qposç»´åº¦: {len(qpos)} x {len(qpos[0]) if qpos else 0}")
            logger.info(f"  - å›¾ç‰‡æ•°é‡: {len(images)}")
            
            return test_data
            
        except Exception as e:
            logger.error(f"æ„å»ºæµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _calculate_test_points(self):
        """è®¡ç®—æµ‹è¯•ç‚¹ï¼Œæ¯test_intervalå¸§ä¸€ä¸ªç‚¹"""
        try:
            total_frames = len(self.df)
            test_points = []
            
            # ä»ç¬¬0å¸§å¼€å§‹ï¼Œæ¯test_intervalå¸§å–ä¸€ä¸ªç‚¹
            for i in range(0, total_frames, self.test_interval):
                # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œactionåºåˆ—æå–ï¼ˆéœ€è¦50å¸§æ¥åŒ¹é…æ¨¡å‹è¾“å‡ºï¼‰
                if i + 50 <= total_frames:
                    test_points.append(i)
            
            logger.info(f"æµ‹è¯•ç‚¹: {test_points}")
            return test_points
            
        except Exception as e:
            logger.error(f"è®¡ç®—æµ‹è¯•ç‚¹å¤±è´¥: {e}")
            return [0]  # é»˜è®¤è‡³å°‘æµ‹è¯•ç¬¬0å¸§
    
    def _extract_action_sequence(self, start_row_index):
        """ä»æŒ‡å®šè¡Œå¼€å§‹æå–è¿ç»­çš„actionåºåˆ—"""
        try:
            if self.df is None:
                return None
            
            # è®¡ç®—ç»“æŸè¡Œç´¢å¼•ï¼ˆå–50è¡Œæˆ–åˆ°æ–‡ä»¶æœ«å°¾ï¼ŒåŒ¹é…æ¨¡å‹è¾“å‡ºç»´åº¦ï¼‰
            end_row_index = min(start_row_index + 50, len(self.df))
            logger.info(f"æå–actionåºåˆ—: ä»ç¬¬{start_row_index}è¡Œåˆ°ç¬¬{end_row_index-1}è¡Œ")
            
            # æå–è¿ç»­çš„actionæ•°æ®
            action_sequence = []
            for i in range(start_row_index, end_row_index):
                row = self.df.iloc[i]
                if 'action' in row:
                    action_data = row['action']
                    # æ£€æŸ¥action_dataæ˜¯å¦æœ‰æ•ˆ
                    if action_data is not None and (isinstance(action_data, np.ndarray) and action_data.size > 0) or (not isinstance(action_data, np.ndarray) and action_data):
                        if isinstance(action_data, np.ndarray):
                            action_sequence.append(action_data.tolist())
                        else:
                            action_sequence.append(action_data)
                    else:
                        logger.warning(f"ç¬¬{i}è¡Œactionæ•°æ®ä¸ºç©º")
                        break
                else:
                    logger.warning(f"ç¬¬{i}è¡Œç¼ºå°‘actionå­—æ®µ")
                    break
            
            if action_sequence:
                logger.info(f"æˆåŠŸæå–{len(action_sequence)}ä¸ªaction")
                return action_sequence
            else:
                logger.warning("æœªæå–åˆ°ä»»ä½•actionæ•°æ®")
                return None
                
        except Exception as e:
            logger.error(f"æå–actionåºåˆ—å¤±è´¥: {e}")
            return None
    
    def _ensure_json_serializable(self, obj):
        """ç¡®ä¿å¯¹è±¡æ˜¯JSONå¯åºåˆ—åŒ–çš„"""
        if isinstance(obj, dict):
            return {key: self._ensure_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._ensure_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj
    
    def test_service_info(self):
        """æµ‹è¯•æœåŠ¡ä¿¡æ¯æ¥å£"""
        try:
            logger.info("æµ‹è¯•æœåŠ¡ä¿¡æ¯æ¥å£...")
            response = requests.get(f"{self.service_url}/info", timeout=10)
            
            if response.status_code == 200:
                info = response.json()
                logger.info("âœ… æœåŠ¡ä¿¡æ¯è·å–æˆåŠŸ")
                logger.info(f"æœåŠ¡åç§°: {info.get('service_name')}")
                logger.info(f"ç‰ˆæœ¬: {info.get('version')}")
                logger.info(f"æ¨¡å‹è·¯å¾„: {info.get('model_info', {}).get('model_path')}")
                return True
            else:
                logger.error(f"âŒ æœåŠ¡ä¿¡æ¯è·å–å¤±è´¥: {response.status_code}")
                return False
                
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æœåŠ¡è¿æ¥å¤±è´¥: {e}")
            return False
    
    def test_inference(self):
        """æµ‹è¯•æ¨ç†æ¥å£"""
        try:
            logger.info("æµ‹è¯•æ¨ç†æ¥å£...")
            logger.info(f"ä½¿ç”¨çœŸå®æ•°æ®æ–‡ä»¶: {self.data_file}")
            
            # å¯¹æ¯ä¸ªæµ‹è¯•ç‚¹è¿›è¡Œæ¨ç†
            for i, test_point in enumerate(self.test_points):
                logger.info(f"\n--- æµ‹è¯•ç‚¹ {i+1}/{len(self.test_points)}: ç¬¬{test_point}å¸§ ---")
                
                # æ„å»ºå½“å‰æµ‹è¯•ç‚¹çš„æ•°æ®
                row_data = self.df.iloc[test_point]
                test_data = self._build_test_data_from_row(row_data, test_point)
                
                if not test_data:
                    logger.error(f"âŒ æµ‹è¯•ç‚¹{test_point}æ•°æ®æ„å»ºå¤±è´¥")
                    continue
                
                # æ‰§è¡Œæ¨ç†
                start_time = time.time()
                response = requests.post(
                    f"{self.service_url}/infer",
                    json=test_data,
                    timeout=100
                )
                # response = requests.get(
                #     f"{self.service_url}/replay",
                #     json=test_data,
                #     timeout=100
                # )
                end_time = time.time()
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get('success'):
                        logger.info(f"âœ… æµ‹è¯•ç‚¹{test_point}æ¨ç†æˆåŠŸ")
                        logger.info(f"å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.3f}ç§’")
                        
                        # è®¡ç®—MSEå¯¹æ¯”
                        actions = result.get('qpos', [])
                        if self.true_action and actions:
                            mse = self._calculate_mse(actions, self.true_action)
                            logger.info(f"ä¸çœŸå®actionçš„MSE: {mse:.6f}")
                            
                            # å¯è§†åŒ–å¯¹æ¯”å¹¶ä¿å­˜åˆ°figsæ–‡ä»¶å¤¹
                            self._visualize_comparison(actions, self.true_action, mse, test_point)
                            
                            # ä¿å­˜æµ‹è¯•ç»“æœ
                            test_result = {
                                'test_point': test_point,
                                'mse': mse,
                                'processing_time': result.get('processing_time', 0),
                                'actions_shape': [len(actions), len(actions[0]) if actions else 0],
                                'visualization_file': f"figs/action_comparison_frame_{test_point:06d}.png"
                            }
                            self.test_results.append(test_result)
                        
                        # ä¿å­˜æ¨ç†ç»“æœåˆ°ç»Ÿä¸€åˆ—è¡¨
                        inference_result = {
                            'test_point': test_point,
                            'predicted_actions': actions,  # æ¨¡å‹æ¨ç†çš„åŠ¨ä½œåºåˆ—
                            'true_actions': self.true_action,  # çœŸå®çš„åŠ¨ä½œåºåˆ—
                            'mse': mse if self.true_action and actions else None,
                            'actions_shape': [len(actions), len(actions[0]) if actions else 0] if actions else None,
                        }
                        self.inference_results.append(inference_result)
                        
                        # åŒæ—¶ä¿å­˜å•ä¸ªæ¨ç†ç»“æœæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
                        self._save_inference_result(result, f"inference_result_{test_point}.json")
                        
                    else:
                        logger.error(f"âŒ æµ‹è¯•ç‚¹{test_point}æ¨ç†å¤±è´¥: {result.get('error')}")
                else:
                    logger.error(f"âŒ æµ‹è¯•ç‚¹{test_point}è¯·æ±‚å¤±è´¥: {response.status_code}")
                    logger.error(f"é”™è¯¯ä¿¡æ¯: {response.text}")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
            
            # ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š
            if self.test_results:
                self._generate_test_report()
                
                # ä¿å­˜æ‰€æœ‰æ¨ç†ç»“æœ
                self.save_all_inference_results()
                
                # ä¿å­˜actionæ•°ç»„
                self.save_actions_as_arrays()
                
                return True
            else:
                logger.error("âŒ æ‰€æœ‰æµ‹è¯•ç‚¹éƒ½å¤±è´¥äº†")
                return False
                
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æ¨ç†è¯·æ±‚å¼‚å¸¸: {e}")
            return False
    
    def _save_inference_result(self, result, filename="inference_result.json"):
        """ä¿å­˜æ¨ç†ç»“æœåˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ… æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¨ç†ç»“æœå¤±è´¥: {e}")
    
    def save_all_inference_results(self, filename=None):
        """ä¿å­˜æ‰€æœ‰æ¨ç†ç»“æœåˆ°ä¸€ä¸ªç»Ÿä¸€çš„JSONæ–‡ä»¶"""
        try:
            if not self.inference_results:
                logger.warning("æ²¡æœ‰æ¨ç†ç»“æœå¯ä¿å­˜")
                return
            
            # ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šçš„è¯
            if filename is None:
                filename = self.output_file
            
            # æ„å»ºå®Œæ•´çš„æ¨ç†ç»“æœæ•°æ®ç»“æ„
            all_results = {
                'metadata': {
                    'service_url': self.service_url,
                    'data_file': self.data_file,
                    'test_interval': self.test_interval,
                    'total_test_points': len(self.test_points),
                    'successful_inferences': len(self.inference_results),
                    'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'test_points': self.test_points
                },
                'inference_results': self.inference_results
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… æ‰€æœ‰æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            logger.info(f"åŒ…å« {len(self.inference_results)} ä¸ªæ¨ç†ç»“æœ")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ‰€æœ‰æ¨ç†ç»“æœå¤±è´¥: {e}")
    
    def save_actions_as_arrays(self, predicted_filename=None, true_filename=None):
        """å°†æ¨ç†actionå’ŒçœŸå®actionä¿å­˜ä¸ºnumpyæ•°ç»„æ ¼å¼ (æ£€æŸ¥ç‚¹æ•°é‡, 14, action chunké•¿åº¦)"""
        try:
            if not self.inference_results:
                logger.warning("æ²¡æœ‰æ¨ç†ç»“æœå¯ä¿å­˜ä¸ºæ•°ç»„")
                return
            
            # ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šçš„è¯
            if predicted_filename is None:
                predicted_filename = self.predicted_actions_file
            if true_filename is None:
                true_filename = self.true_actions_file
            
            import numpy as np
            
            # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç‚¹çš„æ•°æ®
            test_points = []
            predicted_actions_list = []
            true_actions_list = []
            
            for inference_result in self.inference_results:
                test_point = inference_result['test_point']
                predicted_actions = inference_result['predicted_actions']
                true_actions = inference_result['true_actions']
                
                if predicted_actions and true_actions:
                    test_points.append(test_point)
                    predicted_actions_list.append(predicted_actions)
                    true_actions_list.append(true_actions)
            
            if not predicted_actions_list:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„actionæ•°æ®å¯ä¿å­˜")
                return
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            predicted_array = np.array(predicted_actions_list)
            true_array = np.array(true_actions_list)
            
            # æ£€æŸ¥æ•°ç»„å½¢çŠ¶
            logger.info(f"é¢„æµ‹actionæ•°ç»„å½¢çŠ¶: {predicted_array.shape}")
            logger.info(f"çœŸå®actionæ•°ç»„å½¢çŠ¶: {true_array.shape}")
            
            # ä¿å­˜ä¸ºnumpyæ•°ç»„æ–‡ä»¶
            np.save(predicted_filename, predicted_array)
            np.save(true_filename, true_array)
            
            logger.info(f"âœ… é¢„æµ‹actionå·²ä¿å­˜åˆ°: {predicted_filename}")
            logger.info(f"âœ… çœŸå®actionå·²ä¿å­˜åˆ°: {true_filename}")
            
            # åŒæ—¶ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
            actions_summary = {
                'metadata': {
                    'total_checkpoints': len(test_points),
                    'test_points': test_points,
                    'predicted_shape': predicted_array.shape.tolist(),
                    'true_shape': true_array.shape.tolist(),
                    'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
                },
                'predicted_actions': predicted_array.tolist(),
                'true_actions': true_array.tolist()
            }
            
            summary_filename = "actions_summary.json"
            with open(summary_filename, 'w', encoding='utf-8') as f:
                json.dump(actions_summary, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… åŠ¨ä½œæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_filename}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜actionæ•°ç»„å¤±è´¥: {e}")
    
    def _calculate_mse(self, predicted_actions, true_actions):
        """è®¡ç®—é¢„æµ‹actionä¸çœŸå®actionçš„MSE"""
        try:
            import numpy as np
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            pred = np.array(predicted_actions)
            true = np.array(true_actions)
            
            # ç¡®ä¿ç»´åº¦åŒ¹é…
            if pred.shape != true.shape:
                logger.info(f"ç»´åº¦è°ƒæ•´: é¢„æµ‹{pred.shape} vs çœŸå®{true.shape}")
                # å–è¾ƒå°çš„ç»´åº¦è¿›è¡Œå¯¹æ¯”
                min_steps = min(pred.shape[0], true.shape[0])
                pred = pred[:min_steps]
                true = true[:min_steps]
                logger.info(f"è°ƒæ•´ä¸ºå…±åŒç»´åº¦: {min_steps} x {pred.shape[1] if pred.shape[1:] else 0}")
            
            # è®¡ç®—MSE
            mse = np.mean((pred - true) ** 2)
            return mse
            
        except Exception as e:
            logger.error(f"è®¡ç®—MSEå¤±è´¥: {e}")
            return None
    
    def _visualize_comparison(self, predicted_actions, true_actions, mse, test_point):
        """å¯è§†åŒ–é¢„æµ‹actionä¸çœŸå®actionçš„å¯¹æ¯” - æ¯ä¸ªè‡ªç”±åº¦ä¸€å¼ å­å›¾"""
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            pred = np.array(predicted_actions)
            true = np.array(true_actions)
            
            # ç¡®ä¿ç»´åº¦åŒ¹é…
            min_steps = min(pred.shape[0], true.shape[0])
            pred = pred[:min_steps]
            true = true[:min_steps]
            logger.info(f"å¯è§†åŒ–ç»´åº¦: {min_steps} x {pred.shape[1] if pred.shape[1:] else 0}")
            
            # è®¡ç®—å­å›¾å¸ƒå±€ï¼ˆæ¯è¡Œæœ€å¤š4ä¸ªå­å›¾ï¼‰
            num_joints = pred.shape[1]
            cols = min(4, num_joints)
            rows = (num_joints + cols - 1) // cols
            
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))
            fig.suptitle(f'Joint Trajectory Comparison - Frame {test_point} (MSE: {mse:.6f})', fontsize=16)
            
            # ç¡®ä¿axesæ˜¯äºŒç»´æ•°ç»„
            if rows == 1:
                axes = axes.reshape(1, -1)
            if cols == 1:
                axes = axes.reshape(-1, 1)
            
            # ä¸ºæ¯ä¸ªè‡ªç”±åº¦åˆ›å»ºå­å›¾
            for i in range(num_joints):
                row = i // cols
                col = i % cols
                ax = axes[row, col]
                
                # ç»˜åˆ¶é¢„æµ‹å’ŒçœŸå®è½¨è¿¹
                ax.plot(pred[:, i], label='Predicted', color='blue', linewidth=2, alpha=0.8)
                ax.plot(true[:, i], label='Ground Truth', color='red', linestyle='--', linewidth=2, alpha=0.8)
                
                ax.set_title(f'Joint {i+1}')
                ax.set_xlabel('Time Steps')
                ax.set_ylabel('Joint Angle')
                ax.legend()
                ax.grid(True, alpha=0.3)
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(num_joints, rows * cols):
                row = i // cols
                col = i % cols
                axes[row, col].set_visible(False)
            
            plt.tight_layout()
            
            # åˆ›å»ºfigsæ–‡ä»¶å¤¹
            import os
            figs_dir = "figs"
            if not os.path.exists(figs_dir):
                os.makedirs(figs_dir)
                logger.info(f"âœ… åˆ›å»ºfigsæ–‡ä»¶å¤¹: {figs_dir}")
            
            # ä¿å­˜å›¾è¡¨åˆ°figsæ–‡ä»¶å¤¹
            fig_filename = f"action_comparison_frame_{test_point:06d}.png"
            fig_path = os.path.join(figs_dir, fig_filename)
            plt.savefig(fig_path, dpi=300, bbox_inches='tight')
            logger.info(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {fig_path}")
            
            # Display chart (if in environment with display)
            try:
                plt.show()
            except:
                logger.info("Cannot display chart, saved to file")
            
        except ImportError:
            logger.warning("matplotlib not installed, skipping visualization")
        except Exception as e:
            logger.error(f"Visualization failed: {e}")
    
    def _generate_test_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        try:
            if not self.test_results:
                logger.warning("æ²¡æœ‰æµ‹è¯•ç»“æœå¯ç”ŸæˆæŠ¥å‘Š")
                return
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            mse_values = [result['mse'] for result in self.test_results if result['mse'] is not None]
            processing_times = [result['processing_time'] for result in self.test_results]
            
            report = {
                'test_summary': {
                    'total_test_points': len(self.test_points),
                    'successful_tests': len(self.test_results),
                    'test_interval': self.test_interval,
                    'test_points': self.test_points
                },
                'performance_metrics': {
                    'mse_statistics': {
                        'mean_mse': np.mean(mse_values) if mse_values else None,
                        'std_mse': np.std(mse_values) if mse_values else None,
                        'min_mse': np.min(mse_values) if mse_values else None,
                        'max_mse': np.max(mse_values) if mse_values else None
                    },
                    'processing_time_statistics': {
                        'mean_time': np.mean(processing_times) if processing_times else None,
                        'std_time': np.std(processing_times) if processing_times else None,
                        'min_time': np.min(processing_times) if processing_times else None,
                        'max_time': np.max(processing_times) if processing_times else None
                    }
                },
                'detailed_results': self.test_results
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = "test_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            # æ‰“å°æ‘˜è¦
            logger.info("\n" + "="*50)
            logger.info("æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
            logger.info("="*50)
            logger.info(f"æ€»æµ‹è¯•ç‚¹: {len(self.test_points)}")
            logger.info(f"æˆåŠŸæµ‹è¯•: {len(self.test_results)}")
            logger.info(f"æµ‹è¯•é—´éš”: {self.test_interval}å¸§")
            if mse_values:
                logger.info(f"å¹³å‡MSE: {np.mean(mse_values):.6f}")
                logger.info(f"MSEæ ‡å‡†å·®: {np.std(mse_values):.6f}")
            if processing_times:
                logger.info(f"å¹³å‡å¤„ç†æ—¶é—´: {np.mean(processing_times):.3f}ç§’")
            logger.info(f"å¯è§†åŒ–å›¾è¡¨: {len(self.test_results)}ä¸ªï¼Œä¿å­˜åœ¨figs/æ–‡ä»¶å¤¹")
            
            # æ‰“å°åŠ¨ä½œå¯¹æ¯”æ‘˜è¦
            logger.info("\nåŠ¨ä½œå¯¹æ¯”æ‘˜è¦:")
            for i, test_result in enumerate(self.test_results):
                test_point = test_result['test_point']
                mse = test_result['mse']
                actions_shape = test_result['actions_shape']
                logger.info(f"  æµ‹è¯•ç‚¹{test_point}: MSE={mse:.6f}, åŠ¨ä½œå½¢çŠ¶={actions_shape}")
            
            logger.info("="*50)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")
    
    def run_full_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹task_orangeæ¨ç†æµ‹è¯•...")
        logger.info(f"æµ‹è¯•æœåŠ¡åœ°å€: {self.service_url}")
        logger.info(f"æ•°æ®æ–‡ä»¶: {self.data_file}")
        
        # æµ‹è¯•1: æœåŠ¡ä¿¡æ¯
        if not self.test_service_info():
            logger.error("âŒ æœåŠ¡ä¿¡æ¯æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨")
            return False
        
        # æµ‹è¯•2: æ¨ç†åŠŸèƒ½
        if not self.test_inference():
            logger.error("âŒ æ¨ç†æµ‹è¯•å¤±è´¥")
            return False
        
        logger.info("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
    
    def save_test_data(self, filename="test_task_orange_data.json"):
        """ä¿å­˜æµ‹è¯•æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.test_data, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ… æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æµ‹è¯•æ•°æ®å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="task_orangeæ¨ç†æµ‹è¯•ç¨‹åº")
    parser.add_argument(
        "--url", 
        default="http://127.0.0.1:5003",
        help="æœåŠ¡åœ°å€ (é»˜è®¤: http://127.0.0.1:5003)"
    )
    parser.add_argument(
        "--data-file",
        default="/home/admin123/Desktop/pi05_orange/episode_000003.parquet",
        help="æ•°æ®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--save-data",
        action="store_true",
        help="ä¿å­˜æµ‹è¯•æ•°æ®åˆ°æ–‡ä»¶"
    )
    parser.add_argument(
        "--test-interval",
        type=int,
        default=50,
        help="æµ‹è¯•é—´éš”ï¼ˆå¸§æ•°ï¼Œé»˜è®¤: 50ï¼‰"
    )
    parser.add_argument(
        "--output-file",
        default="all_inference_results.json",
        help="æ¨ç†ç»“æœè¾“å‡ºæ–‡ä»¶å (é»˜è®¤: all_inference_results.json)"
    )
    parser.add_argument(
        "--predicted-actions-file",
        default="predicted_actions.npy",
        help="é¢„æµ‹actionæ•°ç»„æ–‡ä»¶å (é»˜è®¤: predicted_actions.npy)"
    )
    parser.add_argument(
        "--true-actions-file",
        default="true_actions.npy",
        help="çœŸå®actionæ•°ç»„æ–‡ä»¶å (é»˜è®¤: true_actions.npy)"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = TaskTester(args.url, args.data_file, args.test_interval, args.output_file, args.predicted_actions_file, args.true_actions_file)
    
    # è¿è¡Œæµ‹è¯•
    success = tester.run_full_test()
    
    # ä¿å­˜æµ‹è¯•æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.save_data:
        tester.save_test_data()
    
    # é€€å‡ºç 
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
